{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUfjn0dBW69k"
      },
      "source": [
        "# Fundamentos de Apache Spark: Funciones avanzadas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DvpJGiGW69s"
      },
      "source": [
        "En este notebook aprenderemos algunas funciones avanzadas para optimizar el rendimiento de Spark o a crear funciones definidas por el usuario (UDF)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmirvN8VFc95",
        "outputId": "74b42bcf-aaa1-4662-8879-fdc6d142ac7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: findspark in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tz6eFyDLW69t"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_-APLiCW69v"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.functions import broadcast\n",
        "from pyspark.sql.types import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zApQov28W69w"
      },
      "source": [
        "### Crea la sesión de SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qC6q8j8xW69w"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHZ5t6G-W69x"
      },
      "source": [
        "### Crear el DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9t10GSMW69x"
      },
      "outputs": [],
      "source": [
        "emp = [(1, \"AAA\", \"dept1\", 1000),\n",
        "    (2, \"BBB\", \"dept1\", 1100),\n",
        "    (3, \"CCC\", \"dept1\", 3000),\n",
        "    (4, \"DDD\", \"dept1\", 1500),\n",
        "    (5, \"EEE\", \"dept2\", 8000),\n",
        "    (6, \"FFF\", \"dept2\", 7200),\n",
        "    (7, \"GGG\", \"dept3\", 7100),\n",
        "    (None, None, None, 7500),\n",
        "    (9, \"III\", None, 4500),\n",
        "    (10, None, \"dept5\", 2500)]\n",
        "\n",
        "dept = [(\"dept1\", \"Department - 1\"),\n",
        "        (\"dept2\", \"Department - 2\"),\n",
        "        (\"dept3\", \"Department - 3\"),\n",
        "        (\"dept4\", \"Department - 4\")]\n",
        "\n",
        "df = spark.createDataFrame(emp, [\"id\", \"name\", \"dept\", \"salary\"])\n",
        "deptdf = spark.createDataFrame(dept, [\"id\", \"name\"])\n",
        "\n",
        "# Create Temp Tables\n",
        "df.createOrReplaceTempView(\"empdf\")\n",
        "deptdf.createOrReplaceTempView(\"deptdf\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTQ-MDD8W69y"
      },
      "source": [
        "### BroadCast Join"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cfml9rS_W69z"
      },
      "source": [
        "El tamaño de la tabla de difusión es de 10 MB. Sin embargo, podemos cambiar el umbral hasta 8GB según la documentación oficial de Spark 2.3.\n",
        "\n",
        "* Podemos verificar el tamaño de la tabla de transmisión de la siguiente manera:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCAPcNIbW69z",
        "outputId": "42928bb0-8182-4def-b9cc-e82f039308e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Default size of broadcast table is 10.0 MB.\n"
          ]
        }
      ],
      "source": [
        "threshold = spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\").rstrip(\"b\")\n",
        "size = int(threshold) / (1024 * 1024)\n",
        "print(\"Default size of broadcast table is {0} MB.\".format(size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5TzWdvWW691"
      },
      "source": [
        "* Podemos establecer el tamaño de la tabla de transmisión para que diga 50 MB de la siguiente manera:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hf0hY0JvW691"
      },
      "outputs": [],
      "source": [
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 50 * 1024 * 1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6USFMo9W691"
      },
      "outputs": [],
      "source": [
        "# Considere que necesitamos unir 2 Dataframes.\n",
        "# small_df: DataFrame pequeño que puede caber en la memoria y es más pequeño que el umbral especificado.\n",
        "# big_df: DataFrame grande que debe unirse con DataFrame pequeño.\n",
        "\n",
        "join_df = big_df.join(broadcast(small_df), big_df[\"id\"] == small_df[\"id\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bh881675W692"
      },
      "source": [
        "### Almacenamiento en caché\n",
        "Podemos usar la función de caché / persistencia para mantener el marco de datos en la memoria. Puede mejorar significativamente el rendimiento de su aplicación Spark si almacenamos en caché los datos que necesitamos usar con mucha frecuencia en nuestra aplicación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWcO3_cEW692",
        "outputId": "7b3696e7-c847-4215-c4d9-97e431f38b25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory Used : True\n",
            "Disk Used : True\n"
          ]
        }
      ],
      "source": [
        "df.cache()\n",
        "df.count()\n",
        "print(\"Memory Used : {0}\".format(df.storageLevel.useMemory))\n",
        "print(\"Disk Used : {0}\".format(df.storageLevel.useDisk))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJjArL9eW692"
      },
      "source": [
        "Cuando usamos la función de caché, usará el nivel de almacenamiento como Memory_Only hasta Spark 2.0.2. Desde Spark 2.1.x es Memory_and_DISK.\n",
        "\n",
        "Sin embargo, si necesitamos especificar los distintos niveles de almacenamiento disponibles, podemos usar el método persist( ). Por ejemplo, si necesitamos mantener los datos solo en la memoria, podemos usar el siguiente fragmento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7cMQseBW692"
      },
      "outputs": [],
      "source": [
        "from pyspark.storagelevel import StorageLevel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_ggn78TW693",
        "outputId": "b622a57e-8492-4ca9-8b91-7a34991d443b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory Used : True\n",
            "Disk Used : False\n"
          ]
        }
      ],
      "source": [
        "deptdf.persist(StorageLevel.MEMORY_ONLY)\n",
        "deptdf.count()\n",
        "print(\"Memory Used : {0}\".format(deptdf.storageLevel.useMemory))\n",
        "print(\"Disk Used : {0}\".format(deptdf.storageLevel.useDisk))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDK2fIWFW693"
      },
      "source": [
        "### No persistir\n",
        "También es importante eliminar la memoria caché de los datos cuando ya no sean necesarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyQhLya1W693",
        "outputId": "8c429dc8-3c63-48d7-ef44-2341929d8850"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, name: string, dept: string, salary: bigint]"
            ]
          },
          "execution_count": 178,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.unpersist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58rA3UPhW694"
      },
      "outputs": [],
      "source": [
        "sqlContext.clearCache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfD4MwuhW694"
      },
      "source": [
        "#  Expresiones SQL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBVvlBIpW694"
      },
      "source": [
        "También podemos usar la expresión SQL para la manipulación de datos. Tenemos la función **expr** y también una variante de un método de selección como **selectExpr** para la e**valuación de expresiones SQL**."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **expr**"
      ],
      "metadata": {
        "id": "qo4FLkKXcpG4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHqFybAvW694",
        "outputId": "a0d512b7-51a7-4e4d-b3f3-fb6113864bc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+----+-----+------+------------+\n",
            "|  id|name| dept|salary|salary_level|\n",
            "+----+----+-----+------+------------+\n",
            "|   1| AAA|dept1|  1000|  low_salary|\n",
            "|   2| BBB|dept1|  1100|  low_salary|\n",
            "|   3| CCC|dept1|  3000|  mid_salary|\n",
            "|   4| DDD|dept1|  1500|  low_salary|\n",
            "|   5| EEE|dept2|  8000| high_salary|\n",
            "|   6| FFF|dept2|  7200| high_salary|\n",
            "|   7| GGG|dept3|  7100| high_salary|\n",
            "|null|null| null|  7500| high_salary|\n",
            "|   9| III| null|  4500|  mid_salary|\n",
            "|  10|null|dept5|  2500|  mid_salary|\n",
            "+----+----+-----+------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import expr\n",
        "\n",
        "# Intentemos categorizar el salario en Bajo, Medio y Alto según la categorización a continuación.\n",
        "\n",
        "# 0-2000: salario_bajo\n",
        "# 2001 - 5000: mid_salary\n",
        "#> 5001: high_salary\n",
        "\n",
        "cond = \"\"\"case when salary > 5000 then 'high_salary'\n",
        "               else case when salary > 2000 then 'mid_salary'\n",
        "                    else case when salary > 0 then 'low_salary'\n",
        "                         else 'invalid_salary'\n",
        "                              end\n",
        "                         end\n",
        "                end as salary_level\"\"\"\n",
        "\n",
        "newdf = df.withColumn(\"salary_level\", expr(cond))\n",
        "newdf.show()\n",
        "\n",
        "\n",
        "\n",
        "#repasar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhnzpdeXW695"
      },
      "source": [
        "### Usando la variante función selectExpr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZuZ18ButW695",
        "outputId": "38848ff2-a7d7-4cc7-c1c1-0f9a2973a0ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+----+-----+------+------------+\n",
            "|  id|name| dept|salary|salary_level|\n",
            "+----+----+-----+------+------------+\n",
            "|   1| AAA|dept1|  1000|  low_salary|\n",
            "|   2| BBB|dept1|  1100|  low_salary|\n",
            "|   3| CCC|dept1|  3000|  mid_salary|\n",
            "|   4| DDD|dept1|  1500|  low_salary|\n",
            "|   5| EEE|dept2|  8000| high_salary|\n",
            "|   6| FFF|dept2|  7200| high_salary|\n",
            "|   7| GGG|dept3|  7100| high_salary|\n",
            "|null|null| null|  7500| high_salary|\n",
            "|   9| III| null|  4500|  mid_salary|\n",
            "|  10|null|dept5|  2500|  mid_salary|\n",
            "+----+----+-----+------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "newdf = df.selectExpr(\"*\", cond)\n",
        "newdf.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdHqfhgRW695"
      },
      "source": [
        "### Funciones definidas por el usuario (UDF)\n",
        "A menudo necesitamos escribir la función en función de nuestro requisito muy específico. Aquí podemos aprovechar las udfs. Podemos escribir nuestras propias funciones en un lenguaje como python y registrar la función como udf, luego podemos usar la función para operaciones de DataFrame.\n",
        "\n",
        "* Función de Python para encontrar el nivel_salario para un salario dado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0m4pnkngW695"
      },
      "outputs": [],
      "source": [
        "def detSalary_Level(sal):\n",
        "    level = None\n",
        "\n",
        "    if(sal > 5000):\n",
        "        level = 'high_salary'\n",
        "    elif(sal > 2000):\n",
        "        level = 'mid_salary'\n",
        "    elif(sal > 0):\n",
        "        level = 'low_salary'\n",
        "    else:\n",
        "        level = 'invalid_salary'\n",
        "    return level"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXDF_WMrW696"
      },
      "source": [
        "* Luego registre la función \"detSalary_Level\" como UDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHemtPu3W696"
      },
      "outputs": [],
      "source": [
        "sal_level = udf(detSalary_Level, StringType())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zalt2lPQW696"
      },
      "source": [
        "* Aplicar función para determinar el salario_level para un salario dado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nrzf_cJGW696",
        "outputId": "fc60bbab-92ab-4e6e-f1e0-7ad457d664c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+----+-----+------+------------+\n",
            "|  id|name| dept|salary|salary_level|\n",
            "+----+----+-----+------+------------+\n",
            "|   1| AAA|dept1|  1000|  low_salary|\n",
            "|   2| BBB|dept1|  1100|  low_salary|\n",
            "|   3| CCC|dept1|  3000|  mid_salary|\n",
            "|   4| DDD|dept1|  1500|  low_salary|\n",
            "|   5| EEE|dept2|  8000| high_salary|\n",
            "|   6| FFF|dept2|  7200| high_salary|\n",
            "|   7| GGG|dept3|  7100| high_salary|\n",
            "|null|null| null|  7500| high_salary|\n",
            "|   9| III| null|  4500|  mid_salary|\n",
            "|  10|null|dept5|  2500|  mid_salary|\n",
            "+----+----+-----+------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "newdf = df.withColumn(\"salary_level\", sal_level(\"salary\"))\n",
        "newdf.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}